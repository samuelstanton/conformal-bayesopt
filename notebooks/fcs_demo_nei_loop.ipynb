{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import botorch\n",
    "\n",
    "sns.set(style='whitegrid', font_scale=1.75)\n",
    "\n",
    "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ac8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsort\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from experiments.std_bayesopt.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_target_grid(bounds, resolution):\n",
    "#     target_dim = bounds.shape[1]\n",
    "#     grid_coords = [np.linspace(bounds[0,i], bounds[1,i], resolution) for i in range(target_dim)]\n",
    "#     target_grid = np.stack(np.meshgrid(*grid_coords), axis=-1)\n",
    "#     target_grid = torch.tensor(target_grid).view(-1, target_dim)\n",
    "#     # target_grid = target_grid.to(val_inputs)\n",
    "#     return target_grid.float()\n",
    "\n",
    "# class ConformalPosterior(botorch.posteriors.Posterior):\n",
    "#     def __init__(self, X, gp, target_bounds, alpha):\n",
    "#         # self.conf_pred_mask = conformal_gp_regression(gp, X, target_grid, alpha)\n",
    "#         self.gp = gp\n",
    "#         self.X = X\n",
    "#         self.target_bounds = target_bounds\n",
    "#         self.alpha = alpha\n",
    "        \n",
    "#     @property\n",
    "#     def device(self):\n",
    "#         return self.X.shape\n",
    "    \n",
    "#     @property\n",
    "#     def dtype(self):\n",
    "#         return self.X.shape\n",
    "    \n",
    "#     @property\n",
    "#     def event_shape(self):\n",
    "#         return self.X.shape[:-2]\n",
    "    \n",
    "#     def rsample(self, sample_shape=()):\n",
    "#         target_grid = generate_target_grid(self.target_bounds, *sample_shape)\n",
    "#         target_grid = target_grid.to(self.X)\n",
    "#         # for later on in the evaluation\n",
    "#         self.gp.conf_pred_mask = conformal_gp_regression(self.gp, self.X, target_grid, self.alpha)\n",
    "#         return target_grid.expand(*self.X.shape[:-1], -1, -1).unsqueeze(0)\n",
    "    \n",
    "# class PassSampler(botorch.sampling.MCSampler):\n",
    "#     def __init__(self, num_samples):\n",
    "#         super().__init__(batch_range=(0, -2))\n",
    "#         self._sample_shape = torch.Size([num_samples])\n",
    "#         self.collapse_batch_dims = True\n",
    "        \n",
    "#     def forward(self, posterior):\n",
    "#         return posterior.rsample(self.sample_shape).transpose(-2, -3)\n",
    "    \n",
    "#     def _construct_base_samples(self, posterior, shape):\n",
    "#         pass\n",
    "    \n",
    "# class ConformalSingleTaskGP(botorch.models.SingleTaskGP):\n",
    "#     is_conformal = False\n",
    "    \n",
    "#     def conformal(self):\n",
    "#         self.is_conformal = True\n",
    "    \n",
    "#     def standard(self):\n",
    "#         self.is_conformal = False\n",
    "        \n",
    "#     def posterior(self, X, observation_noise=False, posterior_transform=None):\n",
    "#         if self.is_conformal:\n",
    "#             # TODO: expose these\n",
    "#             return ConformalPosterior(X, self, torch.tensor([[-2., 2.]]).t(), alpha=0.05)\n",
    "#         else:\n",
    "#             return super().posterior(\n",
    "#                 X = X, observation_noise=observation_noise, posterior_transform=posterior_transform\n",
    "#             )\n",
    "    \n",
    "#     @property\n",
    "#     def batch_shape(self):\n",
    "#         if self.is_conformal:\n",
    "#             try:\n",
    "#                 return self.conf_pred_mask.shape\n",
    "#             except:\n",
    "#                 pass\n",
    "#         return self.train_inputs[0].shape[:-2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpytorch.lazy import DiagLazyTensor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def conformal_gp_regression(gp, test_inputs, target_grid, alpha, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Full conformal Bayes for exact GP regression.\n",
    "#     Args:\n",
    "#         gp (gpytorch.models.GP)\n",
    "#         inputs (torch.Tensor): (batch, q, input_dim)\n",
    "#         target_grid (torch.Tensor): (grid_size, target_dim)\n",
    "#         alpha (float)\n",
    "#     Returns:\n",
    "#         conf_pred_mask (torch.Tensor): (batch, grid_size)\n",
    "#     \"\"\"\n",
    "#     # retraining: condition the GP at every target grid point for every test input\n",
    "#     expanded_inputs = test_inputs.unsqueeze(-3).expand(\n",
    "#         *[-1]*(test_inputs.ndim-2), target_grid.shape[0], -1, -1\n",
    "#     )\n",
    "#     expanded_targets = target_grid.expand(*test_inputs.shape[:-1], -1, -1)\n",
    "#     # the q batch and grid size are flipped\n",
    "#     expanded_targets = expanded_targets.transpose(-2, -3)\n",
    "    \n",
    "#     # cleanup\n",
    "#     gp.train()\n",
    "#     gp.eval() # clear caches\n",
    "#     gp.standard()\n",
    "#     gp.posterior(expanded_inputs)\n",
    "#     gp.conf_pred_mask = None\n",
    "#     gp.conformal()\n",
    "    \n",
    "#     updated_gps = gp.condition_on_observations(expanded_inputs, expanded_targets)\n",
    "    \n",
    "#     # get ready to compute the conformal scores\n",
    "#     train_inputs = updated_gps.train_inputs[0]\n",
    "#     train_labels = updated_gps.prediction_strategy.train_labels\n",
    "# #     print(\"updaed gp:\", updated_gps.prediction_strategy.train_labels.shape, train_inputs.shape)\n",
    "#     train_labels = train_labels.unsqueeze(-1)  # (num_test, grid_size, num_train + 1, target_dim)\n",
    "#     lik_train_train_covar = updated_gps.prediction_strategy.lik_train_train_covar\n",
    "#     prior_mean = updated_gps.prediction_strategy.train_prior_dist.mean.unsqueeze(-1)\n",
    "#     prior_covar = updated_gps.prediction_strategy.train_prior_dist.lazy_covariance_matrix\n",
    "#     noise = updated_gps.likelihood.noise\n",
    "    \n",
    "#     # compute conformal scores (posterior predictive log-likelihood)\n",
    "#     eig_vals, eig_vecs = prior_covar.symeig(eigenvectors=True)  # Q \\Lambda Q^{-1} = K_{XX}\n",
    "#     diag_term = DiagLazyTensor(eig_vals / (eig_vals + noise))  # \\Lambda (\\Lambda + \\sigma I)^{-1}\n",
    "#     lhs = eig_vecs @ diag_term\n",
    "#     mean_rhs = eig_vecs.transpose(-1, -2) @ (train_labels - prior_mean)\n",
    "#     pred_mean = (prior_mean + lhs @ mean_rhs).squeeze(-1)\n",
    "#     covar_rhs = DiagLazyTensor(eig_vals) @ eig_vecs.transpose(-1, -2)\n",
    "#     pred_covar = prior_covar - (lhs @ covar_rhs)\n",
    "#     pred_var = (pred_covar.diag() + noise).clamp(min=1e-6)\n",
    "#     pred_dist = torch.distributions.Normal(pred_mean, pred_var.sqrt())\n",
    "#     conf_scores = pred_dist.log_prob(train_labels.squeeze(-1))\n",
    "    \n",
    "#     ranks_by_score = torch.stack([\n",
    "#         torchsort.soft_rank(\n",
    "#             cc, regularization=\"l2\", regularization_strength=0.1\n",
    "#         ) for cc in conf_scores\n",
    "#     ]) # seems to be returning one indexed values for some reason\n",
    "#     num_total, _ = ranks_by_score.max(-1, keepdim=True)\n",
    "#     norm_ranks = (ranks_by_score) / num_total\n",
    "#     conf_pred_mask = F.threshold(norm_ranks[..., -1], alpha, 0.) / norm_ranks[..., -1]\n",
    "#     return conf_pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02676dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from botorch.acquisition import qExpectedImprovement, qNoisyExpectedImprovement\n",
    "\n",
    "# class qConformalExpectedImprovement(qExpectedImprovement):\n",
    "#     def forward(self, X):\n",
    "#         unconformalized_acqf = super().forward(X) # batch x grid x q\n",
    "#         return (self.model.conf_pred_mask * unconformalized_acqf).sum(-1)\n",
    "    \n",
    "# class qConformalNoisyExpectedImprovement(qNoisyExpectedImprovement):\n",
    "#     def forward(self, X):\n",
    "#         unconformalized_acqf = super().forward(X) # batch x grid x q\n",
    "#         return (self.model.conf_pred_mask * unconformalized_acqf).sum(-1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000695d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fn = lambda x: np.maximum(-0.125 * x ** 2 + 16 * np.sin(x), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285531e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "noise_scale = 1.\n",
    "x_bounds = torch.tensor((-16., 16.)).view(-1, 1)\n",
    "\n",
    "x = np.linspace(*x_bounds, 64)\n",
    "f = obj_fn(x)\n",
    "y = f + noise_scale * np.random.randn(*f.shape)\n",
    "\n",
    "plt.scatter(x, y, edgecolors='black', facecolors='none', label='observations', s=64, zorder=3)\n",
    "plt.plot(x, f, color='black', linestyle='--', label='ground truth', linewidth=2, zorder=2)\n",
    "\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.ylim((-4, 24))\n",
    "plt.legend(loc='upper left', ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botorch\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.acquisition.analytic import UpperConfidenceBound, ExpectedImprovement\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.sampling import IIDNormalSampler\n",
    "\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33012561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambo.utils import DataSplit, update_splits\n",
    "from lambo.optimizers.pymoo import Normalizer\n",
    "\n",
    "cutoff = np.max(np.where(x < -8))\n",
    "x_min, y_min = x.min(0), y.min(0)\n",
    "x_range, y_range = x.max(0) - x_min, y.max(0) - y_min\n",
    "\n",
    "x_norm = Normalizer(\n",
    "    loc=x_min + 0.5 * x_range,\n",
    "    scale=x_range / 2.,\n",
    ")\n",
    "y_norm = Normalizer(\n",
    "    loc=y_min + 0.5 * y_range,\n",
    "    scale=y_range / 2.,\n",
    ")\n",
    "\n",
    "train_x = x[:cutoff]\n",
    "train_y = y[:cutoff]\n",
    "\n",
    "all_inputs = torch.tensor(x_norm(x), device=DEVICE).view(-1, 1)\n",
    "all_targets = torch.tensor(y_norm(y), device=DEVICE).view(-1, 1)\n",
    "target_dim = all_targets.shape[-1]\n",
    "\n",
    "new_split = DataSplit(\n",
    "    all_inputs[:cutoff].cpu().numpy(), all_targets[:cutoff].cpu().numpy()\n",
    ")\n",
    "train_split, val_split, test_split = update_splits(\n",
    "    train_split=DataSplit(),\n",
    "    val_split=DataSplit(),\n",
    "    test_split=DataSplit(),\n",
    "    new_split=new_split,\n",
    "    holdout_ratio=0.2\n",
    ")\n",
    "\n",
    "input_bounds = torch.tensor([-1., 1.], device=DEVICE).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_datashift_opt(splits, acqf=\"ei\"):\n",
    "    def draw_plot(ax):\n",
    "        # plot p(f | x, D)\n",
    "        ax.plot(all_inputs.cpu(), f_hat_mean, color='blue', linewidth=2, zorder=4, label='p(f | x, D)')\n",
    "        ax.fill_between(all_inputs.view(-1).cpu(), f_hat_mean - 1.96 * f_hat_std, f_hat_mean + 1.96 * f_hat_std,\n",
    "                        color='blue', alpha=0.25)\n",
    "\n",
    "        # plot a(x)\n",
    "        ax.plot(all_inputs.cpu(), acq_vals, color='green', zorder=5, linewidth=2, label='a(x)')\n",
    "        ax.scatter(input_query.cpu(), target_query.cpu(), marker='x', color='red', label='x*', zorder=5,\n",
    "                   s=32, linewidth=2)\n",
    "\n",
    "        # plot observed\n",
    "        ax.scatter(train_inputs.cpu(), train_targets.cpu(), edgecolors='black', facecolors='black',\n",
    "                   label='D', s=32, zorder=3)\n",
    "\n",
    "        # plot true function\n",
    "        ax.plot(all_inputs.cpu(), y_norm(f), color='black', linestyle='--', label='f')\n",
    "\n",
    "        ax.set_ylim((-2., 2.))\n",
    "\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        return ax\n",
    "\n",
    "    train_split, val_split, test_split = splits\n",
    "    \n",
    "    if acqf == 'ei':\n",
    "        acqf_init = lambda gp, best_f: qExpectedImprovement(\n",
    "            gp,\n",
    "            best_f=train_targets.max(0)[0],\n",
    "            sampler=IIDNormalSampler(64)\n",
    "        )\n",
    "    elif acqf == \"nei\":\n",
    "        acqf_init = lambda gp, best_f: qNoisyExpectedImprovement(gp, X_baseline=train_inputs, sampler=IIDNormalSampler(64))\n",
    "    elif acqf == 'conformal_ei':\n",
    "        def acqf_init(gp, best_f):\n",
    "            gp.conformal()\n",
    "            return qConformalExpectedImprovement(\n",
    "                gp,\n",
    "                best_f=train_targets.max(0)[0],\n",
    "                sampler=PassSampler(32),\n",
    "                cache_root=False\n",
    "            )\n",
    "    elif acqf == \"conformal_nei\":\n",
    "        def acqf_init(gp, best_f):\n",
    "            gp.conformal()\n",
    "            return qConformalNoisyExpectedImprovement(\n",
    "                gp,\n",
    "                X_baseline=train_inputs,\n",
    "                sampler=PassSampler(32),\n",
    "                cache_root=False\n",
    "            )\n",
    "        \n",
    "    num_rounds = 32\n",
    "    plot_interval = 8\n",
    "    \n",
    "    queried_targets = []\n",
    "    for round_idx in range(num_rounds):\n",
    "        train_inputs = torch.tensor(train_split[0], device=DEVICE)\n",
    "        train_targets = torch.tensor(train_split[1], device=DEVICE)\n",
    "\n",
    "        matern_gp = ConformalSingleTaskGP(\n",
    "            train_X=train_inputs,\n",
    "            train_Y=train_targets,\n",
    "            alpha=0.1,\n",
    "            conformal_bounds=torch.tensor([[-2., 2.]]).t(),\n",
    "        ).to(DEVICE)\n",
    "        mll = ExactMarginalLogLikelihood(matern_gp.likelihood, matern_gp)\n",
    "        fit_gpytorch_model(mll)\n",
    "        # acq_fn = UpperConfidenceBound(matern_gp, beta=8.)\n",
    "        # HERE WE USE EI\n",
    "        # acq_fn = ExpectedImprovement(matern_gp, best_f=train_targets.max())\n",
    "        acq_fn = acqf_init(matern_gp, train_targets.max())\n",
    "\n",
    "        matern_gp.requires_grad_(False)\n",
    "        matern_gp.eval()\n",
    "        with torch.no_grad():\n",
    "            f_hat_dist = matern_gp(all_inputs)\n",
    "            y_hat_dist = matern_gp.likelihood(f_hat_dist)\n",
    "            f_hat_mean = f_hat_dist.mean.cpu()\n",
    "            f_hat_std = f_hat_dist.variance.sqrt().cpu()\n",
    "            y_hat_mean = f_hat_mean.cpu()\n",
    "            y_hat_std = y_hat_dist.variance.sqrt().cpu()\n",
    "            acq_vals = acq_fn(all_inputs[:, None]).cpu()\n",
    "\n",
    "        input_query = optimize_acqf(acq_fn, input_bounds, 1, num_restarts=4, raw_samples=16)[0]\n",
    "        x_query = x_norm.inv_transform(input_query.cpu().numpy())\n",
    "        f_query = obj_fn(x_query)\n",
    "        y_query = f_query + noise_scale * np.random.randn(*f_query.shape)\n",
    "        target_query = torch.tensor(y_norm(y_query), device=DEVICE)\n",
    "\n",
    "        if round_idx % plot_interval == 0:\n",
    "            print(f'{train_split[0].shape[0]} train, {val_split[0].shape[0]} val, {test_split[0].shape[0]} test')\n",
    "            fig = plt.figure(figsize=(8, 5))\n",
    "            ax = fig.add_subplot(1, 1, 1)\n",
    "            draw_plot(ax)\n",
    "\n",
    "        new_split = DataSplit(\n",
    "            input_query.reshape(-1, 1).cpu(),\n",
    "            target_query.reshape(-1, 1).cpu(),\n",
    "        )\n",
    "        train_split, val_split, test_split = update_splits(\n",
    "            train_split, val_split, test_split, new_split, holdout_ratio=0.2\n",
    "        )\n",
    "        queried_targets.append(target_query)\n",
    "    plt.show()\n",
    "    return torch.tensor(queried_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f91447",
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal_pts = [\n",
    "    run_datashift_opt([train_split, val_split, test_split], acqf=\"conformal_ei\") for _ in range(2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf145d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_pts = [run_datashift_opt([train_split, val_split, test_split], acqf=\"ei\") for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal = torch.stack(conformal_pts).cummax(1)[0]\n",
    "std = torch.stack(std_pts).cummax(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(conformal.mean(0), label = \"conformal qEI\")\n",
    "plt.fill_between(torch.arange(32), \n",
    "                 conformal.mean(0) - 2. / 5**0.5 * conformal.std(0), \n",
    "                 conformal.mean(0) + 2. / 5**0.5 * conformal.std(0), \n",
    "                 alpha = 0.3)\n",
    "plt.plot(std.mean(0), label = \"qEI\")\n",
    "plt.fill_between(torch.arange(32), \n",
    "                 std.mean(0) - 2. / 5**0.5 * std.std(0), \n",
    "                 std.mean(0) + 2. / 5**0.5 * std.std(0), \n",
    "                 alpha = 0.3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Function Evaluations\")\n",
    "plt.ylabel(\"Best Achieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c06e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
